# =====================================================
# TÜRKIYE ÖZEL DERS PİYASASI SCRAPER
# GitHub Actions Workflow - Bi-Weekly Schedule
# =====================================================

name: Bi-Weekly Tutor Market Scraper

on:
  # 2 haftada bir Pazartesi saat 04:00 UTC (Türkiye 07:00)
  schedule:
    - cron: '0 4 * * 1'  # Her Pazartesi 04:00 UTC
  
  # Manuel tetikleme
  workflow_dispatch:
    inputs:
      platform:
        description: 'Platform to scrape'
        required: true
        default: 'ozelders'
        type: choice
        options:
          - ozelders
          - all
      dry_run:
        description: 'Dry run (no database writes)'
        required: false
        default: false
        type: boolean

# İlk çalıştırma tarihi (2 haftalık hesaplama için)
env:
  FIRST_RUN_DATE: '2026-01-20'
  PYTHON_VERSION: '3.11'

jobs:
  # =====================================================
  # JOB 1: 2 Haftalık Kontrol
  # =====================================================
  check-biweekly:
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    
    steps:
      - name: Check if bi-weekly run
        id: check
        run: |
          # Manuel tetikleme her zaman çalışsın
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "Manual trigger - will run"
            exit 0
          fi
          
          # 2 haftalık hesaplama
          start=$(date -d "${{ env.FIRST_RUN_DATE }}" +%s)
          now=$(date +%s)
          
          # Hafta farkını hesapla
          days_diff=$(( (now - start) / 86400 ))
          week_diff=$(( days_diff / 7 ))
          week_index=$(( week_diff % 2 ))
          
          echo "Days since first run: $days_diff"
          echo "Weeks since first run: $week_diff"
          echo "Week index (0=run, 1=skip): $week_index"
          
          if [ $week_index -eq 0 ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "Bi-weekly check passed - will run"
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "Bi-weekly check failed - skipping this week"
          fi

  # =====================================================
  # JOB 2: Ana Scraping İşlemi
  # =====================================================
  scrape:
    needs: check-biweekly
    if: needs.check-biweekly.outputs.should_run == 'true'
    runs-on: ubuntu-22.04  # Ubuntu 22.04 - Playwright ile daha uyumlu
    timeout-minutes: 360   # 6 saat timeout (anti-spam delays nedeniyle uzun sürecek)
    
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_KEY }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt
      
      - name: Install Playwright with dependencies
        run: |
          playwright install --with-deps chromium
      
      - name: Validate environment
        run: |
          if [ -z "$SUPABASE_URL" ]; then
            echo "Error: SUPABASE_URL is not set"
            exit 1
          fi
          if [ -z "$SUPABASE_SERVICE_ROLE_KEY" ]; then
            echo "Error: SUPABASE_KEY is not set"
            exit 1
          fi
          echo "Environment validated successfully"
      
      - name: Run scraper with retry
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 300  # 5 saat per attempt
          max_attempts: 2       # 2 deneme yeterli
          retry_wait_seconds: 600  # 10 dakika bekle
          command: |
            cd scraper
            DRY_RUN_FLAG=""
            if [ "${{ github.event.inputs.dry_run }}" == "true" ]; then
              DRY_RUN_FLAG="--dry-run"
            fi
            python scraper.py --platform ${{ github.event.inputs.platform || 'ozelders' }} $DRY_RUN_FLAG 2>&1 | tee scraper_output.log
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            scraper/*.log
          retention-days: 30
      
      - name: Extract statistics
        if: always()
        id: stats
        run: |
          if [ -f scraper/scraper_output.log ]; then
            TOTAL=$(grep -oP 'Total listings: \K\d+' scraper/scraper_output.log 2>/dev/null || echo "0")
            NEW=$(grep -oP 'New listings: \K\d+' scraper/scraper_output.log 2>/dev/null || echo "0")
            UPDATED=$(grep -oP 'Updated listings: \K\d+' scraper/scraper_output.log 2>/dev/null || echo "0")
            ERRORS=$(grep -oP 'Errors: \K\d+' scraper/scraper_output.log 2>/dev/null || echo "0")
            STATUS=$(grep -oP 'Status: \K\w+' scraper/scraper_output.log 2>/dev/null || echo "unknown")
          else
            TOTAL="0"; NEW="0"; UPDATED="0"; ERRORS="0"; STATUS="no_log"
          fi
          
          echo "total=$TOTAL" >> $GITHUB_OUTPUT
          echo "new=$NEW" >> $GITHUB_OUTPUT
          echo "updated=$UPDATED" >> $GITHUB_OUTPUT
          echo "errors=$ERRORS" >> $GITHUB_OUTPUT
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          
          echo "### Scraping Results" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Status | $STATUS |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Listings | $TOTAL |" >> $GITHUB_STEP_SUMMARY
          echo "| New Listings | $NEW |" >> $GITHUB_STEP_SUMMARY
          echo "| Updated Listings | $UPDATED |" >> $GITHUB_STEP_SUMMARY
          echo "| Errors | $ERRORS |" >> $GITHUB_STEP_SUMMARY

  # =====================================================
  # JOB 3: Notification (Opsiyonel)
  # =====================================================
  notify:
    needs: [check-biweekly, scrape]
    if: always() && needs.check-biweekly.outputs.should_run == 'true'
    runs-on: ubuntu-latest
    
    steps:
      - name: Send notification on failure
        if: needs.scrape.result == 'failure'
        run: |
          echo "Scraping failed! Check the logs."
      
      - name: Log success
        if: needs.scrape.result == 'success'
        run: |
          echo "Scraping completed successfully!"
